---
title: "IBM HR Analysis Project Notebook"
author: "Minghao Du, Mengying Zhao, Wenlu Chen, Haitong Ni, Haoyu Wang"
output:
  rmdformats::downcute:
    self_contained: true
    thumbnails: true
    df_print: paged
    lightbox: true
    gallery: true
    toc_depth: 3
    highlight: tango
    code_folding: "show"
    code_download: false
params:
  date: !r Sys.Date()
---

```{css echo=FALSE}
@import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500;600;700&display=swap');
code{
  font-family: 'Fira Code'!important;
}
```
```{r include=FALSE}
    # html_document:
    #   theme: Superhero 
    #   toc: true
    #   toc_float:
    #     collapsed: false
    #   toc_depth: 3
    #   df_print: paged
    #   code_folding: hide
    #   keep_md: true
library(rmarkdown)
if (!require(kableExtra)) install.packages("kableExtra")
library(kableExtra)
if (!require(rmdformats)) install.packages("rmdformats")
library(rmdformats)
library(knitr)
library(magrittr)
```

## Intro

### Set-up

Load packages: 
```{r message=FALSE, warning=FALSE}
rm(list = ls())
if (!require(readxl)) install.packages("readxl")
library(readxl)
if (!require(tidyverse)) install.packages("tidyverse")
library(tidyverse)
if (!require(ggplot2)) install.packages("ggplot2")
library(ggplot2)
# theme_set(theme_classic())
theme_set(theme_bw())
if (!require(ggExtra)) install.packages("ggExtra")
library(ggExtra)


if (!require(e1071)) install.packages("e1071")
library(e1071)
if (!require(fastDummies)) install.packages("fastDummies")
library(fastDummies)
if (!require(caret)) install.packages("caret")
library(caret)
if (!require(tree)) install.packages('tree')
library(tree)
if (!require(rpart)) install.packages('rpart')
library(rpart)
library(rpart.plot)
if (!require(progress)) install.packages("progress")
library(progress)
if (!require(randomForest)) install.packages("randomForest")
library(randomForest)
if (!require(randomForestExplainer)) install.packages("randomForestExplainer")
library(randomForestExplainer)
if (!require(pROC)) install.packages('pROC')
library(pROC)
```

### Import Data

```{R}
raw_data <-
  read.csv(
    "../data/WA_Fn-UseC_-HR-Employee-Attrition.csv",
    stringsAsFactors = T,
    na.strings = '?'
  )

names(raw_data)[names(raw_data) == 'ï..Age'] <- 'Age'

glimpse(raw_data)
```

### Preprocess the data 

```{R}

tmp_Attrition <- raw_data$Attrition

data_droped_col <-
  subset(raw_data,
         select = -c(Attrition, EmployeeCount, StandardHours, Over18))

data_dummied <- dummy_cols(data_droped_col)

data_processed <-
  data_dummied[, sapply(data_dummied, class) != "factor"]

colnames(data_processed) <- gsub(" ", "", colnames(data_processed))
colnames(data_processed) <- gsub("-", "", colnames(data_processed))
colnames(data_processed) <- gsub("&", "", colnames(data_processed))
colnames(data_processed) <- gsub("`", "", colnames(data_processed))

data_processed$Attrition <- tmp_Attrition

glimpse(data_processed)
```

```{R}
set.seed(10)

split <- sample(nrow(data_processed), nrow(data_processed) * 0.75)

data_train <- data_processed[split,]
data_test <- data_processed[-split,]

```

### Understanding the data 

#### Histograms
```{r}
raw_data %>%
  ggplot(aes(x = Attrition)) +
  geom_bar()

raw_data %>%
  ggplot(aes(x = Age)) +
  geom_histogram(binwidth = 10, aes(fill = Attrition))

raw_data %>%
  ggplot(aes(x = MonthlyIncome)) +
  geom_histogram(binwidth = 1000, aes(fill = Attrition))

raw_data %>%
  ggplot(aes(x = OverTime)) +
  geom_bar(aes(fill = Attrition))

raw_data %>%
  ggplot(aes(Department)) + 
  geom_bar(aes(fill = Attrition))

raw_data %>%
  ggplot(aes(Gender)) + 
  geom_bar(aes(fill = Attrition))
```

#### Scatterplot
```{r message=FALSE, warning=FALSE}

incomeAge <- raw_data %>%
  ggplot(aes(MonthlyIncome, Age)) +
  geom_point(aes(color = Attrition)) +
  geom_smooth(method = "gam", se = F) +
  theme(legend.position = "bottom")
incomeAge %>%  ggMarginal(type = "histogram", groupFill = T)

# (
#   raw_data %>%
#     ggplot(aes(JobSatisfaction, JobLevel)) +
#     geom_jitter(aes(color = Attrition)) +
#     geom_smooth(method = "gam", se = F) + 
#     theme(legend.position = "bottom")
# ) %>%
#   ggMarginal(type = "histogram", groupFill = T)
```

```{r message=FALSE, warning=FALSE}

workyear <- raw_data %>%
  ggplot(aes(TotalWorkingYears, YearsAtCompany)) +
  geom_point(aes(color = Attrition)) +
  geom_smooth(method = "gam", se = F) +
  theme(legend.position = "bottom")
workyear %>%  ggMarginal(type = "histogram", groupFill = T)

# (
#   raw_data %>%
#     ggplot(aes(JobSatisfaction, JobLevel)) +
#     geom_jitter(aes(color = Attrition)) +
#     geom_smooth(method = "gam", se = F) + 
#     theme(legend.position = "bottom")
# ) %>%
#   ggMarginal(type = "histogram", groupFill = T)
```

## Modeling 

### Decision Tree

#### build a model with a training set
```{r}
training_model <- rpart(Attrition ~ .,
                        data = data_train,
                        method = "class")

rpart.plot(training_model)
```

#### k-fold Cross-validation，cp with 0

```{r}
full_tree <- rpart(
  Attrition ~ .,
  data = data_train,
  method = "class",
  control = rpart.control(cp = 0) #cp=0, more complex, let the tree grow
)  
rpart.plot(full_tree)
```

```{r}
printcp(full_tree)   # xerror, xstd - cross validation results  
```

```{r}
plotcp(full_tree)    
```

```{r}
min_xerror <-
  full_tree$cptable[which.min(full_tree$cptable[, "xerror"]), ]
min_xerror

# prune tree with minimum cp value
min_xerror_tree <- prune(full_tree, cp = min_xerror[1])
rpart.plot(min_xerror_tree)
```

```{r}
bp_tree <- min_xerror_tree
data_test$ct_pred_prob <- predict(bp_tree, data_test)[, 2]
data_test$ct_pred_class <- ifelse(data_test$ct_pred_prob > 0.5, "Yes", "No")

dt_table <-
  table(data_test$ct_pred_class,
        data_test$Attrition,
        dnn = c("predicted", "actual"))
confusionMatrix(dt_table)
```

### Random Forest 

#### baseline forest
```{r}
treeCount <- 1000

data.forest <- randomForest(
  Attrition ~ .,
  data = data_train,
  ntree = treeCount,
  importance = T,
  localImp = T
)
plot(data.forest)
```

#### testing hyper peramiter: mtry
```{r}
data.forest <- randomForest(
  Attrition ~ .,
  data = data_train,
  ntree = treeCount,
  mtry = ncol(data_processed) - 1,
  importance = T,
  localImp = T
)
data.forest
#+ fig.width=11, fig.height=8
plot(data.forest)
```

```{r fig.height=10, fig.width=10}
data.frame(importance(data.forest))
varImpPlot(data.forest)
```

```{r}
data.forest.pred <- predict(data.forest, data_test, typr = "class")
rf_confMtx <- table(data.forest.pred, data_test$Attrition)
confusionMatrix(rf_confMtx)
```

#### Brute force search for best mtry
```{r}
pb <- progress_bar$new(format = ":elapsedfull [:bar] :current/:total (:percent)",
                       total = (ncol(data_train) - 1))

mtry.list <- c()
data.forest.final <- NULL
max <- 0
for (i in 1:(ncol(data_train) - 1)) {
  data.forest.dummy <- randomForest(
    Attrition ~ .,
    data = data_train,
    ntree = treeCount,
    mtry = i,
    importance = T,
    localImp = T
  )
  val <- predict(data.forest.dummy, data_test, type = "class")
  score <- mean(val == data_test$Attrition)
  if (score >= max) {
    data.forest.final <- data.forest.dummy
    max <- score
  }
  mtry.list[i] <- score
  pb$tick()
}

plot(mtry.list)

data.forest.final
plot(data.forest.final)
```

```{r fig.height=10, fig.width=10}
data.frame(importance(data.forest.final))

varImpPlot(data.forest.final)

data_test$rf_pred_prob <- predict(data.forest.final, data_test, type="prob")[,2]
data_test$rf_pred_class <- predict(data.forest.final, data_test)

rf_confMtx <- table(data_test$rf_pred_class, data_test$Attrition)
confusionMatrix(rf_confMtx)
```


#### Plot forest predict 

Plotting the best forest's predictions in associate with top 5 most important independent variables
```{r message=FALSE, warning=FALSE}
imp <- data.frame(importance(data.forest.final))

imp <- imp[order(-imp$MeanDecreaseAccuracy), ]

impName <- row.names(imp)

impNameComp <- combn(impName[1:5], 2)

apply(impNameComp, MARGIN = 2, FUN = function(i) {
  plot_predict_interaction(data.forest.final, data_test, i[1], i[2])
})

```

```{r eval=FALSE, include=FALSE}
# auto generate random forest model report
explain_forest(
  data.forest.final,
  path = paste0(getwd(), "/HR_Analysis_forest_explained.html"),
  interactions = TRUE,
  no_of_pred_plots = 10, 
  data = data_test
)
```

### SVM

```{R}
svm_1 <-
  svm(
    Attrition ~ .,
    data = data_train,
    method = "C-classification",
    kernal = "radial"
  )

summary(svm_1)
```

```{R}
svm_1_pred <- predict(svm_1, data_test)

xtab_svm_1 <- table(Actual = data_test$Attrition, Predicted = svm_1_pred)

confusionMatrix(xtab_svm_1)
```

```{R}

svm_tune <-
  tune.svm(
    Attrition ~ .,
    data = data_train,
    kernel = "radial",
    cost = 10 ^ (-5:4),
    gamma = c(seq(1, 10, 4) %o% 10 ^ (-4:1))
  )

svm_tune$performances
```

```{R}
best_svm_mod <- svm_tune$best.model

data_test$svm_pred_class <- predict(best_svm_mod, data_test)
data_test$svm_dv <- as.numeric(attr(predict(best_svm_mod, data_test, decision.value = TRUE),"decision.values"))

xtab_svm_best <-
  table(Actual = data_test$Attrition, Predicted = data_test$svm_pred_class)

confusionMatrix(xtab_svm_best)
```


## Evaluation 

### Decision Tree
```{r}
confusionMatrix(dt_table)
```

### Random Forest 
```{r}
confusionMatrix(rf_confMtx)
```

### SVM
```{r}
confusionMatrix(xtab_svm_best)
```

### ROC & AUC
```{r message=FALSE, warning=FALSE}
ct_roc <- roc(data_test$Attrition, data_test$ct_pred_prob, auc = TRUE)
rf_roc <- roc(data_test$Attrition, data_test$rf_pred_prob, auc = TRUE)
svm_roc <- roc(data_test$Attrition, data_test$svm_dv, auc = TRUE)

plot(ct_roc, print.auc = TRUE, col = "blue")
plot(rf_roc, print.auc = TRUE, print.auc.y = .4, col = "green", add = TRUE)
plot(svm_roc, print.auc = TRUE, print.auc.y = .3, col = "black", add = TRUE)
```
